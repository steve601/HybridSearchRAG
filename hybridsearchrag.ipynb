{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10592145,"sourceType":"datasetVersion","datasetId":6555670}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we're gonna implement RAG using hybrid search,**First**,after splitting our documents into chunks,we'll create vector similarity retriever using their embeddings and also create keyword/syntatic retirever, then after that we combine the retrievers using Ensemble retriever in such a way that when a user query, all the outputs from the two retrievers will be considered and passed to the LLM for answer generation","metadata":{}},{"cell_type":"markdown","source":"## Installing dependancies","metadata":{}},{"cell_type":"code","source":"!pip install --quiet langchain langchain_community # popular framework for generative ai\n%pip install --upgrade --quiet huggingface_hub\n!pip install --quiet faiss-cpu # vectorstore\n!pip install --quiet pypdf # loader in rag\n!pip install --quiet langchain_huggingface\n!pip install --quiet chromadb # vectorstore\n!pip install --quiet langchain_core\n!pip install --quiet rank_bm25","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:25:35.234158Z","iopub.execute_input":"2025-03-28T05:25:35.234457Z","iopub.status.idle":"2025-03-28T05:26:02.361176Z","shell.execute_reply.started":"2025-03-28T05:25:35.234433Z","shell.execute_reply":"2025-03-28T05:26:02.360230Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from langchain.llms import HuggingFaceHub \nfrom langchain_huggingface import HuggingFaceEndpoint # for accessing huggingface models\nfrom langchain_huggingface import HuggingFaceEmbeddings # embeding the documents in the vectorstore\nfrom langchain_huggingface import ChatHuggingFace # chat model\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS,Chroma\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain.retrievers import BM25Retriever\nfrom langchain.retrievers import EnsembleRetriever","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:26:13.703443Z","iopub.execute_input":"2025-03-28T05:26:13.703799Z","iopub.status.idle":"2025-03-28T05:26:14.690008Z","shell.execute_reply.started":"2025-03-28T05:26:13.703766Z","shell.execute_reply":"2025-03-28T05:26:14.689333Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Let's first load our data, it's a pdf format so we use PyPDFLoader","metadata":{}},{"cell_type":"code","source":"pdfloader = PyPDFLoader('/kaggle/input/intro-todatascience/datascience.pdf')\ndocs = pdfloader.load()\n# their are a quiet number of document loaders, for more info check https://python.langchain.com/docs/integrations/document_loaders/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:26:16.303538Z","iopub.execute_input":"2025-03-28T05:26:16.304060Z","iopub.status.idle":"2025-03-28T05:26:29.624414Z","shell.execute_reply.started":"2025-03-28T05:26:16.304025Z","shell.execute_reply":"2025-03-28T05:26:29.623745Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## We split our document into chunks","metadata":{}},{"cell_type":"code","source":"splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\ntexts = splitter.split_documents(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:26:31.882888Z","iopub.execute_input":"2025-03-28T05:26:31.883338Z","iopub.status.idle":"2025-03-28T05:26:31.912310Z","shell.execute_reply.started":"2025-03-28T05:26:31.883313Z","shell.execute_reply":"2025-03-28T05:26:31.911378Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Now we create embeddings for vector-similarity search retriever","metadata":{}},{"cell_type":"code","source":"embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\ndb = Chroma.from_documents(texts,embedding=embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:26:33.176096Z","iopub.execute_input":"2025-03-28T05:26:33.176381Z","iopub.status.idle":"2025-03-28T05:26:46.719022Z","shell.execute_reply.started":"2025-03-28T05:26:33.176359Z","shell.execute_reply":"2025-03-28T05:26:46.718285Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"vector_retriever = db.as_retriever(search_type='similarity',search_kwargs = {'k':5})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:26:54.144803Z","iopub.execute_input":"2025-03-28T05:26:54.145539Z","iopub.status.idle":"2025-03-28T05:26:54.149306Z","shell.execute_reply.started":"2025-03-28T05:26:54.145509Z","shell.execute_reply":"2025-03-28T05:26:54.148510Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Then we create our syntatic retriever, we use BM25Retriever\n**BM25** is a probabilistic ranking function that scores documents based on their relevance to a given query. It is an improvement over the traditional TF-IDF (Term Frequency-Inverse Document Frequency) model, incorporating term saturation and document length normalization.\nIt looks at how often your search words appear in a document and considers the document’s length to provide the most relevant results.\nIt precomputes term frequencies, document lengths, and inverse document frequencies.\nWhen a query is issued, BM25Retriever calculates the BM25 score for each document in the corpus.\nThe documents are ranked based on their BM25 scores.\nThe top k most relevant documents are returned.","metadata":{}},{"cell_type":"markdown","source":"![Online Image](https://www.kopp-online-marketing.com/wp-content/uploads/2024/05/Screenshot-2024-06-14-093457-e1718354274506.png)","metadata":{}},{"cell_type":"code","source":"keyword_retriever = BM25Retriever.from_documents(documents=texts,k=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:27:03.650593Z","iopub.execute_input":"2025-03-28T05:27:03.650958Z","iopub.status.idle":"2025-03-28T05:27:03.684588Z","shell.execute_reply.started":"2025-03-28T05:27:03.650928Z","shell.execute_reply":"2025-03-28T05:27:03.683981Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Now we combine the two retrievers using Ensemble Retriever\n**Ensemble Retriever** is a retrieval mechanism that combines multiple retrievers to improve document retrieval accuracy. Instead of relying on a single retrieval method (like BM25 or dense retrieval), it aggregates results from different retrievers to enhance performance.\n\nThere are types od ensemble retrievers,common ones are **Rank Fusion (Reciprocal Rank Fusion - RRF)** and **Score Fusion (Weighted Average)**.\n\nIn this notebook we implement **Score Fusion (Weighted Average)**.","metadata":{}},{"cell_type":"code","source":"ensemble_retriever = EnsembleRetriever(\n    retrievers=[keyword_retriever, vector_retriever],\n    weights=[0.5, 0.5]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:28:12.762964Z","iopub.execute_input":"2025-03-28T05:28:12.763288Z","iopub.status.idle":"2025-03-28T05:28:12.767485Z","shell.execute_reply.started":"2025-03-28T05:28:12.763266Z","shell.execute_reply":"2025-03-28T05:28:12.766440Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## We load our LLM model and define a promt to it","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_tkn = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n\nllm = HuggingFaceEndpoint(\n    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n    task=\"text-generation\",\n    max_new_tokens=512,\n    do_sample=False,\n    repetition_penalty=1.03,\n    huggingfacehub_api_token=hf_tkn\n)\n\nchat_model = ChatHuggingFace(llm=llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:30:33.755984Z","iopub.execute_input":"2025-03-28T05:30:33.756359Z","iopub.status.idle":"2025-03-28T05:30:35.444602Z","shell.execute_reply.started":"2025-03-28T05:30:33.756329Z","shell.execute_reply":"2025-03-28T05:30:35.443700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96bf865c69d74460af7414a341cf475b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a776b2a258ed4d9cbc4ebf31ecd41896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ff7ffcfbe34bc0818ef08de8791bc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7225c8d2a940e39465918cdf39ac8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa69a16531e9447d8f055242eef2220c"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"prompt_text = \"\"\"You are an assistant who is an expert in question-answering tasks.\n                Answer the following question using only the following pieces of \n                retrieved context.\n                If the answer is not in the context, do not make up answers, just \n                say that you don't know.\n                Keep the answer detailed and well formatted based on the \n                information from the context.\n                Do not begin with 'Based on the provided context...' and capitalize\n                the first letter.\n                \n                Question:\n                {question}\n                \n                Context:\n                {context}\n                \n                Answer:\n            \"\"\"\n\nrag_prompt_template = ChatPromptTemplate.from_template(prompt_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:33:58.367050Z","iopub.execute_input":"2025-03-28T05:33:58.367448Z","iopub.status.idle":"2025-03-28T05:33:58.371745Z","shell.execute_reply.started":"2025-03-28T05:33:58.367421Z","shell.execute_reply":"2025-03-28T05:33:58.370917Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs) # for formatting the output\n# # RunnablePassthrough allows us to pass the user's question to the prompt and model\nretrieval_chain = (\n                {\"context\":(ensemble_retriever | format_docs),\"question\":RunnablePassthrough()}\n                | rag_prompt_template\n                | chat_model\n                | StrOutputParser()\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:49:12.347503Z","iopub.execute_input":"2025-03-28T05:49:12.347810Z","iopub.status.idle":"2025-03-28T05:49:12.352934Z","shell.execute_reply.started":"2025-03-28T05:49:12.347788Z","shell.execute_reply":"2025-03-28T05:49:12.352133Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Let's test our RAG, wow! it's pretty good","metadata":{}},{"cell_type":"code","source":"query = \"Data science allows us to adopt four different strategies to explore the world using data,state them\"\nretrieval_chain.invoke(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:49:15.385207Z","iopub.execute_input":"2025-03-28T05:49:15.385508Z","iopub.status.idle":"2025-03-28T05:49:20.087119Z","shell.execute_reply.started":"2025-03-28T05:49:15.385485Z","shell.execute_reply":"2025-03-28T05:49:20.086354Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Data science allows us to explore the world using data through four different strategies: 1. Probing reality, where data can be gathered using passive or active methods; 2. Understanding people and the world, which is currently beyond the scope of most companies and individuals but is being heavily researched by large companies and governments in areas such as natural language understanding, computer vision, psychology, and neuroscience; 3. Predicting future events by using past data to reveal patterns and natural clusters that simplify problem-solving; and 4. Making informed decisions by adopting evidence-based approaches that consider all available data. These four strategies are not mutually exclusive, and they can be used individually or in combination with each other. The democratization of data analysis, facilitated by cloud computing and open-source development, has made it possible for individuals and small companies to access the same analytical tools and techniques previously available only to large tech giants. This paradigm shift, referred to as the data science revolution, represents a disruptive change in our society caused by the process of datification, which involves quantifying aspects of the world that were previously not measured. Data science is not a new scientific discipline, but its applications are being driven by advancements in technology and datification, which are transforming the way we gather, analyze, and apply data to take informed decisions.'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}